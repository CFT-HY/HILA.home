---
layout: page
title: About HILA project and Our Goal 
description: The Vision, Mission and Developing Philosophy
background: '/img/AxionStringNetwork.jpg'
---
<p> As an user, living in digitalized era, where even hand-holding devices rely on 
    <a herf="https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture">Heterogeneous System Architecture</a> (HSA), is wonderful and enjoyable in many ways.
    The computing powers provided both by multiple CPU cores and 
    <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation">Streaming Multiprocessors</a> from GPUs pleasure us as users while also give 
    us many challenges as developers of CFT simulation softwares.   
</p>

<p class="font-italic"> Routines, which could release us from struggling on computing technologies with HSA 
     and then allow us focus on innovating modeling and algorithms, 
    are always expected and welcomed.   
</p>

<p class="font-weight-bold">This is what HILA project could offer. </p>

<p>Developed under <a href="https://en.wikipedia.org/wiki/C%2B%2B14">modern C++</a> standards, HILA is a 
    <a href="https://en.wikipedia.org/wiki/Lattice_field_theory">Lattice Field Theory</a> programming framework, targeted at HPC simulations.
    Design features of HILA include (1) comprehensive class templates of commonly used mathematical and physical objects, such as matrix, which
    generally work as operators of Lie algebra for non-Abelian Lie groups. 
    A carefully implemented field class template over lattice can accept tensors with arbitrary rank. 
    This make writing CFT applications straightforward and intuitive through HILA's APIs. 
    (2) Depending on the hardware platform, optimized executables for specific hardware architectures/protocols will be produced. 
    Details of the implementations of parallelization on different computing architecture are hidden from the HILA's user,   
    this means same application source codes built upon HILA's APTs could automatically run on platforms with different hardware architectures.
    This potability is supported on <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a> parallelization, 
    <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GP-GPU</a> parallelization 
    as well as <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX vectorization</a> vectorization.
</p>

<p class="mb-5">Hila is based on hila preprocessor "hilapp", which is a C++ source-to-source transformer using the 
    <a href="https://clang.llvm.org/docs/LibTooling.html">libtooling</a> toolbox of the
    <a href="https://clang.llvm.org">Clang</a> compiler.
    It converts application C++ to platform-specific C++ code,
    which is passed to appropriate compilers for the platforms.
</p>

<p>Behind the scenes hila takes care of MPI layout and communications.  It lays out the 
    lattice fields differently for different computing platforms: 'array of structures' (standard),
    'array of structures of vectors' (AVX-type), or 'structure of arrays' (GPU-type).</p>    


<div class="clearfix">
    <a class="btn btn-primary float-right" href="{{ site.docs_url }}">Go to Documentations</span> &rarr;</a>
</div>